"""
@file knowledge_graph.py
@brief çŸ¥è¯†å›¾è°±è®¿é—®ä¸å®ä½“ä¸Šä¸‹æ–‡æŠ½å–æ¨¡å—ï¼ŒåŸºäº SPARQL å°è£… Wikidata ç­‰çŸ¥è¯†å›¾è°±ã€‚
       Knowledge graph access & entity context extraction utilities based on SPARQL (e.g. Wikidata).
"""

from __future__ import annotations

import json
import re
import threading
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Mapping, Optional, Sequence

import asyncio
from concurrent.futures import ThreadPoolExecutor, Future

import requests
from SPARQLWrapper import SPARQLWrapper, JSON  # type: ignore[import-untyped]

from kan.utils.logging import get_logger

logger = get_logger(__name__)


# ============================================================
# é…ç½® Configuration
# ============================================================


@dataclass
class KnowledgeGraphConfig:
    """
    @brief çŸ¥è¯†å›¾è°±å®¢æˆ·ç«¯é…ç½®ã€‚Configuration for the knowledge graph client.
    @param endpoint_url SPARQL ç«¯ç‚¹åœ°å€ï¼Œä¾‹å¦‚ Wikidata endpointã€‚
           SPARQL endpoint URL, e.g. Wikidata public endpoint.
    @param user_agent HTTP User-Agentï¼Œç”¨äºç¤¼è²Œè®¿é—®å…¬å…±æœåŠ¡ã€‚
           HTTP User-Agent header used for polite access to public services.
    @param timeout è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ã€‚Request timeout in seconds.
    @param max_neighbors æ¯ä¸ªå®ä½“æœ€å¤šè¿”å›å¤šå°‘ä¸€è·³é‚»å±…ã€‚Max number of 1-hop neighbors per entity.
    @param cache_dir é‚»å±…ä¸è¡¨é¢å½¢å¼ç¼“å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºå‡å°‘é‡å¤è¿œç¨‹è¯·æ±‚ã€‚
           Optional cache directory for neighbor / surface caches to reduce remote calls.
    @param language æŸ¥è¯¢ label / æœç´¢æ—¶ä½¿ç”¨çš„è¯­è¨€ä»£ç ï¼ˆå¦‚ "zh"ã€"en"ï¼‰ã€‚
           Language code used when searching for labels, e.g. "zh", "en".
    @param max_workers çº¿ç¨‹æ± æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼Œç”¨äºå¹¶å‘ç½‘ç»œ I/Oã€‚
           Max worker threads for the internal thread pool used for concurrent network I/O.
    """

    endpoint_url: str = "https://query.wikidata.org/sparql"
    user_agent: str = "KAN-KG-Client/0.1"
    timeout: int = 30
    max_neighbors: int = 32
    cache_dir: Optional[str] = "data/kg_cache"
    language: str = "zh"
    max_workers: int = 20
    neighbor_query_mode: str = "full"


# ============================================================
# å®¢æˆ·ç«¯å®ç° Client implementation
# ============================================================


class KnowledgeGraphClient:
    """
    @brief åŸºäº SPARQL çš„çŸ¥è¯†å›¾è°±å®¢æˆ·ç«¯ï¼Œæä¾›å®ä½“é“¾æ¥ä¸å®ä½“ä¸Šä¸‹æ–‡æŠ½å–æ¥å£ã€‚
           SPARQL-based knowledge graph client with entity linking & context extraction APIs.
    """

    def __init__(self, cfg: KnowledgeGraphConfig) -> None:
        """
        @brief åˆå§‹åŒ–çŸ¥è¯†å›¾è°±å®¢æˆ·ç«¯ã€‚Initialize the knowledge graph client.
        @param cfg KnowledgeGraphConfig é…ç½®å¯¹è±¡ã€‚Configuration object.
        """
        self.cfg = cfg

        # ä¸ºæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘æŸ¥è¯¢ï¼Œä¸å†è·¨çº¿ç¨‹å¤ç”¨åŒä¸€ä¸ª SPARQLWrapper å®ä¾‹ã€‚
        # To support multi-threaded queries, we no longer reuse a single SPARQLWrapper instance.
        self._sparql: Optional[SPARQLWrapper] = (
            None  # kept only for potential future use
        )

        # é‚»å±…ç¼“å­˜ï¼šå®ä½“ ID -> é‚»å±… ID åˆ—è¡¨ï¼ˆå†…å­˜çº§ï¼‰ã€‚
        # Neighbor cache: entity ID -> list of neighbor IDs (in-memory).
        self._neighbor_cache: Dict[str, List[str]] = {}

        # è¡¨é¢å½¢å¼ç¼“å­˜ï¼šsurface string -> QID åˆ—è¡¨ï¼ˆå†…å­˜çº§ï¼‰ã€‚
        # Surface cache: surface form -> list of QIDs (in-memory).
        self._surface_cache: Dict[str, List[str]] = {}

        # ç¼“å­˜é”ï¼šä¿è¯å¤šçº¿ç¨‹è¯»å†™å®‰å…¨ã€‚
        # Locks for caches to ensure thread-safe access in multi-threaded environments.
        self._surface_lock = threading.Lock()
        self._neighbor_lock = threading.Lock()

        # ç½‘ç»œ I/O çº¿ç¨‹æ± ï¼ˆæƒ°æ€§åˆ›å»ºï¼‰ã€‚
        # Thread pool for network I/O (created lazily).
        self._executor: Optional[ThreadPoolExecutor] = None

        # ç¼“å­˜ SPARQLWrapper çš„çº¿ç¨‹å±€éƒ¨å­˜å‚¨ï¼ˆæ¯ä¸ªçº¿ç¨‹ä¸€ä¸ªå®ä¾‹ï¼Œæ—¢çº¿ç¨‹å®‰å…¨åˆèƒ½å¤ç”¨è¿æ¥ï¼‰
        # Thread-local storage for SPARQLWrapper instances (one per thread).
        self._sparql_local = threading.local()

        # å‡†å¤‡ç¼“å­˜ç›®å½•å¹¶å°è¯•åŠ è½½è¡¨é¢å½¢å¼ç¼“å­˜ã€‚
        # Prepare cache directory and try to load surface cache from disk.
        if self.cfg.cache_dir is not None:
            cache_path = Path(self.cfg.cache_dir)
            cache_path.mkdir(parents=True, exist_ok=True)
            self._load_surface_cache()

    # ------------------------------------------------------------
    # èµ„æºç®¡ç†ï¼šçº¿ç¨‹æ± ä¸ SPARQL
    # ------------------------------------------------------------

    def _get_executor(self) -> ThreadPoolExecutor:
        """
        @brief è·å–ï¼ˆæˆ–æƒ°æ€§åˆ›å»ºï¼‰å†…éƒ¨çº¿ç¨‹æ± ï¼Œç”¨äºå¹¶å‘ç½‘ç»œ I/Oã€‚
               Get (or lazily create) the internal thread pool for concurrent network I/O.
        @return ThreadPoolExecutor å®ä¾‹ã€‚ThreadPoolExecutor instance.
        """
        if self._executor is None:
            self._executor = ThreadPoolExecutor(
                max_workers=self.cfg.max_workers,
                thread_name_prefix="kg-io",
            )
        return self._executor

    def close(self) -> None:
        """
        @brief æ˜¾å¼å…³é—­å†…éƒ¨çº¿ç¨‹æ± ç­‰èµ„æºã€‚
               Explicitly shutdown the internal thread pool and related resources.
        @note
            - å»ºè®®åœ¨é•¿æ—¶é—´è®­ç»ƒç»“æŸåè°ƒç”¨ï¼Œä»¥åŠ å¿«è¿›ç¨‹é€€å‡ºã€‚
              It is recommended to call this after long-running training to accelerate process exit.
        """
        if self._executor is not None:
            self._executor.shutdown(wait=False)
            self._executor = None

    def __del__(self) -> None:  # pragma: no cover - best-effort cleanup
        """
        @brief ææ„æ—¶å°è¯•å›æ”¶èµ„æºï¼ˆå°½åŠ›è€Œä¸ºï¼‰ã€‚
               Best-effort resource cleanup when the object is being destroyed.
        """
        try:
            self.close()
        except Exception:  # noqa: BLE001
            # åœ¨ææ„ä¸­ç»ä¸æŠ›å‡ºå¼‚å¸¸ã€‚
            # Never raise exceptions from __del__.
            pass

    def _get_sparql(self) -> SPARQLWrapper:
        """
        @brief è·å–å½“å‰çº¿ç¨‹ä¸“å±çš„ SPARQLWrapper å®ä¾‹ï¼ˆçº¿ç¨‹å®‰å…¨ä¸”å¤ç”¨è¿æ¥ï¼‰ã€‚
               Get a per-thread SPARQLWrapper instance (thread-safe and reusing connections).
        @return SPARQLWrapper å¯¹è±¡ã€‚SPARQLWrapper instance.
        """
        local = self._sparql_local
        sp = getattr(local, "sparql", None)
        if sp is None:
            sp = SPARQLWrapper(self.cfg.endpoint_url)
            sp.setReturnFormat(JSON)
            sp.setTimeout(self.cfg.timeout)
            sp.addCustomHttpHeader("User-Agent", self.cfg.user_agent)
            # å¦‚æœ endpoint æ”¯æŒ keep-aliveï¼Œå¤ç”¨ä¸€ä¸ªå®ä¾‹å¯ä»¥å°½é‡å¤ç”¨åº•å±‚è¿æ¥
            # Reuse the underlying HTTP connection if the endpoint supports keep-alive.
            local.sparql = sp
        return sp

    # ------------------------------------------------------------
    # ç¼“å­˜å·¥å…·ï¼šè·¯å¾„ä¸æŒä¹…åŒ–
    # ------------------------------------------------------------

    def _cache_path(self, name: str) -> Path:
        """
        @brief æ„é€ ç¼“å­˜æ–‡ä»¶è·¯å¾„ã€‚Build the cache file path.
        @param name ç¼“å­˜æ–‡ä»¶åï¼ˆä¸å«ç›®å½•ï¼‰ã€‚Cache file name (without directory).
        @return å®Œæ•´ Path å¯¹è±¡ã€‚Full Path object for the cache file.
        """
        if self.cfg.cache_dir is None:
            # æ²¡æœ‰æ˜¾å¼ cache_dir æ—¶ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•ä¸‹çš„ä¸´æ—¶è·¯å¾„ã€‚
            # Use current working directory as a fallback when cache_dir is None.
            return Path(name)
        return Path(self.cfg.cache_dir) / name

    def _load_neighbors_from_disk(self, entity_id: str) -> Optional[List[str]]:
        """
        @brief ä»ç£ç›˜åŠ è½½å•ä¸ªå®ä½“é‚»å±…ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚
               Load cached neighbors for a single entity from disk if present.
        @param entity_id å®ä½“ IDã€‚Entity ID.
        @return é‚»å±… ID åˆ—è¡¨æˆ– Noneã€‚List of neighbor IDs or None.
        """
        if self.cfg.cache_dir is None:
            return None
        path = self._cache_path(f"{entity_id}.neighbors.json")
        if not path.exists():
            return None
        try:
            with path.open("r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, list):
                return [str(x) for x in data]
        except Exception as e:  # noqa: BLE001
            logger.warning("Failed to load neighbor cache for %s: %s", entity_id, e)
        return None

    def _save_neighbors_to_disk(self, entity_id: str, neighbors: Sequence[str]) -> None:
        """
        @brief å°†å•ä¸ªå®ä½“çš„é‚»å±…åˆ—è¡¨ä¿å­˜åˆ°ç£ç›˜ç¼“å­˜ã€‚
               Save the neighbor list for a single entity to disk.
        @param entity_id å®ä½“ IDã€‚Entity ID.
        @param neighbors é‚»å±… ID åºåˆ—ã€‚Neighbor ID sequence.
        """
        if self.cfg.cache_dir is None:
            return
        path = self._cache_path(f"{entity_id}.neighbors.json")
        try:
            with path.open("w", encoding="utf-8") as f:
                json.dump(list(neighbors), f, ensure_ascii=False)
        except Exception as e:  # noqa: BLE001
            logger.warning("Failed to save neighbor cache for %s: %s", entity_id, e)

    def _load_surface_cache(self) -> None:
        """
        @brief å°è¯•ä»ç£ç›˜åŠ è½½è¡¨é¢å½¢å¼ç¼“å­˜ã€‚
               Try to load the surface-form cache from disk.
        """
        if self.cfg.cache_dir is None:
            return
        path = self._cache_path("surface_cache.json")
        if not path.exists():
            return
        try:
            with path.open("r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, Mapping):
                new_cache: Dict[str, List[str]] = {}
                for k, v in data.items():
                    if isinstance(v, list):
                        new_cache[str(k)] = [str(x) for x in v]
                # ç”¨é”ä¿æŠ¤èµ‹å€¼ï¼Œé¿å…å¹¶å‘ä¸‹ç›´æ¥æ›¿æ¢å¼•ç”¨å¯¼è‡´å¥‡æ€ªé—®é¢˜
                # Assign under lock to avoid races.
                with self._surface_lock:
                    self._surface_cache = new_cache
                logger.info(
                    "Loaded surface cache with %d entries", len(self._surface_cache)
                )
        except Exception as e:  # noqa: BLE001
            logger.warning("Failed to load surface cache: %s", e)

    def _save_surface_cache(self) -> None:
        """
        @brief å°†å½“å‰è¡¨é¢å½¢å¼ç¼“å­˜ä¿å­˜åˆ°ç£ç›˜ã€‚
               Persist the current surface-form cache to disk.
        """
        if self.cfg.cache_dir is None:
            return
        path = self._cache_path("surface_cache.json")
        try:
            # ğŸ”’ åœ¨é”å†…æ‹·è´ä¸€ä»½å¿«ç…§ï¼Œé¿å…éå†è¿‡ç¨‹ä¸­å­—å…¸è¢«å…¶ä»–çº¿ç¨‹ä¿®æ”¹ã€‚
            # Take a snapshot under the lock to avoid "dictionary changed size during iteration".
            with self._surface_lock:
                data = dict(self._surface_cache)

            # åœ¨é”å¤–æ‰§è¡Œç£ç›˜å†™å…¥ï¼Œå‡å°‘é”æŒæœ‰æ—¶é—´ã€‚
            # Do disk I/O outside the lock to minimize lock contention.
            with path.open("w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False)
        except Exception as e:  # noqa: BLE001
            logger.warning("Failed to save surface cache: %s", e)

    def _build_neighbor_query(self, entity_id: str) -> str:
        """
        @brief æ ¹æ®é…ç½®æ„é€ é‚»å±…æŸ¥è¯¢ç”¨çš„ SPARQLã€‚
               Build SPARQL query for neighbor retrieval according to config.
        @param entity_id Wikidata å®ä½“ IDï¼Œä¾‹å¦‚ "Q76"ã€‚
               Wikidata entity ID, e.g. "Q76".
        @return SPARQL æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚SPARQL query string.
        """
        mode = (self.cfg.neighbor_query_mode or "full").lower()

        if mode == "direct":
            # åªä½¿ç”¨ direct properties (wdt:)ï¼ŒåªæŸ¥å‡ºè¾¹ï¼šwd:Q ?p ?neighbor
            # This is much lighter than scanning both directions and all property namespaces.
            return f"""
            PREFIX wd: <http://www.wikidata.org/entity/>
            PREFIX wdt: <http://www.wikidata.org/prop/direct/>

            SELECT DISTINCT ?neighbor WHERE {{
              wd:{entity_id} ?p ?neighbor .
              FILTER(isIRI(?neighbor))
              FILTER(STRSTARTS(STR(?p), STR(wdt:)))
            }}
            LIMIT {int(self.cfg.max_neighbors)}
            """

        # é»˜è®¤æ¨¡å¼ï¼šä¿æŒåŸæœ‰è¯­ä¹‰ï¼ˆåŒå‘ + ä»»æ„è°“è¯ï¼‰
        return f"""
        PREFIX wd: <http://www.wikidata.org/entity/>

        SELECT DISTINCT ?neighbor WHERE {{
          {{
            wd:{entity_id} ?p ?neighbor .
          }}
          UNION
          {{
            ?neighbor ?p wd:{entity_id} .
          }}
          FILTER(isIRI(?neighbor))
        }}
        LIMIT {int(self.cfg.max_neighbors)}
        """

    # ------------------------------------------------------------
    # Wikidata æœç´¢æ¥å£ï¼ˆåŒæ­¥ & å¼‚æ­¥ï¼‰
    # ------------------------------------------------------------

    def _search_wikidata(self, surface: str, limit: int = 1) -> List[str]:
        """
        @brief ä½¿ç”¨ Wikidata çš„æœç´¢ APIï¼Œå°†è¡¨é¢å½¢å¼æ˜ å°„ä¸ºä¸€ä¸ªæˆ–å¤šä¸ªå®ä½“ IDã€‚
               Use Wikidata search API to map a surface form to one or more entity IDs.
        @param surface è¡¨é¢å­—ç¬¦ä¸²ã€‚Surface form string.
        @param limit è¿”å›çš„æœ€å¤§å®ä½“æ•°é‡ã€‚Maximum number of entity IDs to return.
        @return åŒ¹é…åˆ°çš„å®ä½“ ID åˆ—è¡¨ã€‚List of matched entity IDs.
        """
        surface = surface.strip()
        if not surface:
            return []

        # å…ˆæŸ¥å†…å­˜ç¼“å­˜ï¼ˆåŠ é”ï¼‰ã€‚
        # First consult in-memory cache (with lock).
        with self._surface_lock:
            cached = self._surface_cache.get(surface)
        if cached is not None:
            return cached[:limit]

        # æ„é€  Wikidata wbsearchentities è¯·æ±‚ã€‚
        params = {
            "action": "wbsearchentities",
            "format": "json",
            "language": self.cfg.language,
            "search": surface,
            "limit": max(1, int(limit)),
        }
        url = "https://www.wikidata.org/w/api.php"

        try:
            resp = requests.get(
                url,
                params=params,
                timeout=self.cfg.timeout,
                headers={"User-Agent": self.cfg.user_agent},
            )
            resp.raise_for_status()
            data: Dict[str, Any] = resp.json()
        except Exception as e:  # noqa: BLE001
            logger.warning("Wikidata search failed for %r: %s", surface, e)
            # å¤±è´¥æ—¶ä»ç„¶åˆå§‹åŒ–ç©ºåˆ—è¡¨ï¼Œé¿å…é‡å¤æ‰“åŒä¸€ä¸ªå¤±è´¥è¯·æ±‚ã€‚
            with self._surface_lock:
                self._surface_cache.setdefault(surface, [])
            return []

        results: List[str] = []
        for item in data.get("search", []):
            qid = item.get("id")
            if isinstance(qid, str) and qid.startswith("Q"):
                results.append(qid)

        with self._surface_lock:
            self._surface_cache.setdefault(surface, results)

        self._save_surface_cache()

        return results[:limit]

    async def asearch_wikidata(self, surface: str, limit: int = 1) -> List[str]:
        """
        @brief å¼‚æ­¥ç‰ˆæœ¬çš„ Wikidata æœç´¢æ¥å£ï¼ˆåŸºäºçº¿ç¨‹æ± å°è£…ï¼‰ã€‚
               Async version of Wikidata search API, backed by a thread pool.
        @param surface è¡¨é¢å­—ç¬¦ä¸²ã€‚Surface form string.
        @param limit è¿”å›çš„æœ€å¤§å®ä½“æ•°é‡ã€‚Maximum number of entity IDs to return.
        @return åŒ¹é…åˆ°çš„å®ä½“ ID åˆ—è¡¨ã€‚List of matched entity IDs.
        @note
            - å†…éƒ¨å¤ç”¨åŒæ­¥å®ç° _search_wikidataï¼Œè¡Œä¸ºä¿æŒä¸€è‡´ã€‚
              Internally reuse the sync implementation _search_wikidata, keeping behavior identical.
        """
        loop = asyncio.get_running_loop()
        executor = self._get_executor()
        return await loop.run_in_executor(
            executor, self._search_wikidata, surface, limit
        )

    # ------------------------------------------------------------
    # å®ä½“é“¾æ¥ï¼šä» tokens åˆ° QID åˆ—è¡¨
    # ------------------------------------------------------------

    def link_entities_from_tokens(self, tokens: Sequence[str]) -> List[str]:
        """
        @brief åŸºäºåˆ†è¯ç»“æœæ‰§è¡Œå®ä½“é“¾æ¥ï¼Œå°† token åºåˆ—æ˜ å°„ä¸º Wikidata å®ä½“ IDã€‚
               Perform entity linking from token sequence to Wikidata entity IDs.
        @param tokens æ–‡æœ¬åˆ†è¯åçš„ token åºåˆ—ã€‚Token sequence produced by text preprocessing.
        @return å»é‡åçš„å®ä½“ ID åˆ—è¡¨ã€‚Deduplicated list of entity IDs.
        @note
            - å½“å‰å®ç°ä½¿ç”¨ç®€å•çš„é€ token è§„åˆ™ï¼Œæ¯ä¸ª token å•ç‹¬æŸ¥è¯¢ Wikidata æœç´¢ APIã€‚
              The current implementation uses a simple per-token rule: each token is searched individually.
        """
        entities: List[str] = []
        seen: set[str] = set()

        for tok in tokens:
            tok = tok.strip()
            # è¿‡æ»¤æ‰é•¿åº¦ 1 çš„æ— æ„ä¹‰ tokenï¼Œä¾‹å¦‚æ ‡ç‚¹æˆ–å•ä¸ªæ±‰å­—ã€‚
            if len(tok) <= 1:
                continue

            # å¯ä»¥è§†éœ€è¦æ·»åŠ æ­£åˆ™è¿‡æ»¤ï¼ˆä¾‹å¦‚åªä¿ç•™å­—æ¯/æ•°å­—/æ±‰å­—ï¼‰ã€‚
            if not re.search(r"[0-9A-Za-z\u4e00-\u9fff]", tok):
                continue

            qids = self._search_wikidata(tok, limit=1)
            for qid in qids:
                if qid not in seen:
                    seen.add(qid)
                    entities.append(qid)

        logger.info(
            "link_entities_from_tokens: %d tokens -> %d entities",
            len(tokens),
            len(entities),
        )
        return entities

    async def alink_entities_from_tokens(self, tokens: Sequence[str]) -> List[str]:
        """
        @brief å¼‚æ­¥ç‰ˆæœ¬ï¼šåŸºäºåˆ†è¯ç»“æœçš„å®ä½“é“¾æ¥æ¥å£ã€‚
               Async version of entity linking from token sequence.
        @param tokens æ–‡æœ¬åˆ†è¯åçš„ token åºåˆ—ã€‚Token sequence obtained from text preprocessing.
        @return å»é‡åçš„å®ä½“ ID åˆ—è¡¨ã€‚Deduplicated list of entity IDs.
        @note
            - å†…éƒ¨ä»ç„¶è°ƒç”¨åŒæ­¥ link_entities_from_tokensï¼Œé€šè¿‡çº¿ç¨‹æ±  offloadã€‚
              Internally calls sync link_entities_from_tokens via the thread pool.
        """
        loop = asyncio.get_running_loop()
        executor = self._get_executor()
        return await loop.run_in_executor(
            executor, self.link_entities_from_tokens, list(tokens)
        )

    # ------------------------------------------------------------
    # å®ä½“é“¾æ¥ï¼šä»åŸå§‹æ–‡æœ¬åˆ° QID åˆ—è¡¨ï¼ˆä¿ç•™æ¥å£ï¼‰
    # ------------------------------------------------------------

    def link_entities(self, text: str) -> List[str]:
        """
        @brief ä»åŸå§‹æ–‡æœ¬æ‰§è¡Œå®ä½“é“¾æ¥ï¼ˆç®€å•åˆ†è¯ + å®ä½“é“¾æ¥ï¼‰ï¼Œä¸ºå…¼å®¹ä¿ç•™ã€‚
               Entity linking from raw text (simple tokenization + entity linking), kept for compatibility.
        @param text åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²ã€‚Raw text string.
        @return å®ä½“ ID åˆ—è¡¨ã€‚List of entity IDs.
        @note
            - åœ¨æ–°ç‰ˆæµæ°´çº¿ä¸­å»ºè®®å…ˆç”±ä¸Šæ¸¸é¢„å¤„ç†æ¨¡å—å®Œæˆåˆ†è¯ï¼Œå†è°ƒç”¨ link_entities_from_tokensã€‚
              In the new pipeline, it is recommended that upstream preprocessing handles tokenization explicitly
              and then calls link_entities_from_tokens.
        """
        # æç®€ whitespace åˆ†è¯ï¼Œåªåšå…œåº•ï¼›æ­£å¼æµç¨‹ä¸åº”ä¾èµ–è¯¥é€»è¾‘ã€‚
        tokens = re.findall(r"\S+", text)
        return self.link_entities_from_tokens(tokens)

    # ------------------------------------------------------------
    # é‚»å±…æŸ¥è¯¢ï¼ˆåŒæ­¥ & å¼‚æ­¥ï¼‰
    # ------------------------------------------------------------

    def get_neighbors(self, entity_id: str) -> List[str]:
        """
        @brief è·å–å•ä¸ªå®ä½“åœ¨çŸ¥è¯†å›¾è°±ä¸­çš„ä¸€è·³é‚»å±…å®ä½“ IDã€‚
               Get 1-hop neighbor entity IDs for a single entity in the KG.
        @param entity_id Wikidata å®ä½“ IDï¼Œä¾‹å¦‚ "Q76"ã€‚
               Wikidata entity ID, e.g. "Q76".
        @return é‚»å±…å®ä½“ ID åˆ—è¡¨ï¼ˆä¸å«è‡ªèº«ï¼‰ï¼Œé•¿åº¦ä¸è¶…è¿‡ max_neighborsã€‚
                List of neighbor entity IDs (excluding self), up to max_neighbors.
        """
        # å†…å­˜ç¼“å­˜ä¼˜å…ˆï¼ˆåŠ é”ï¼‰ã€‚
        # Prefer in-memory cache (with lock).
        with self._neighbor_lock:
            cached = self._neighbor_cache.get(entity_id)
        if cached is not None:
            return cached

        # ç£ç›˜ç¼“å­˜å…¶æ¬¡ã€‚
        # Then consult disk cache.
        disk_cached = self._load_neighbors_from_disk(entity_id)
        if disk_cached is not None:
            with self._neighbor_lock:
                self._neighbor_cache[entity_id] = disk_cached
            return disk_cached

        # å‘é€ SPARQL æŸ¥è¯¢ï¼ˆå½“å‰çº¿ç¨‹å¤ç”¨è‡ªå·±çš„ SPARQLWrapper å®ä¾‹ï¼Œä»¥æ”¯æŒå¤šçº¿ç¨‹ä¸”å¤ç”¨è¿æ¥ï¼‰ã€‚
        # Send SPARQL query (reuse the per-thread SPARQLWrapper instance for multi-threading & connection reuse).
        sparql = self._get_sparql()
        query = self._build_neighbor_query(entity_id)
        sparql.setQuery(query)

        try:
            results = sparql.query().convert()
        except Exception as e:  # noqa: BLE001
            logger.error("SPARQL query failed for %s: %s", entity_id, e)
            with self._neighbor_lock:
                self._neighbor_cache[entity_id] = []
            return []

        neighbors: List[str] = []
        for row in results.get("results", {}).get("bindings", []):
            uri = row.get("neighbor", {}).get("value")
            if not uri:
                continue
            qid = str(uri).rsplit("/", 1)[-1]
            if qid and qid != entity_id:
                neighbors.append(qid)

        neighbors = sorted(set(neighbors))

        # å›å†™å†…å­˜ç¼“å­˜ & ç£ç›˜ç¼“å­˜ã€‚
        # Write back to in-memory and disk caches.
        with self._neighbor_lock:
            self._neighbor_cache[entity_id] = neighbors
        self._save_neighbors_to_disk(entity_id, neighbors)

        logger.info("get_neighbors: %s -> %d neighbors", entity_id, len(neighbors))
        return neighbors

    async def aget_neighbors(self, entity_id: str) -> List[str]:
        """
        @brief å¼‚æ­¥è·å–å•ä¸ªå®ä½“çš„ä¸€è·³é‚»å±…ã€‚
               Async version of get_neighbors for a single entity.
        @param entity_id å®ä½“ IDã€‚Entity ID.
        @return é‚»å±…å®ä½“ ID åˆ—è¡¨ã€‚List of neighbor entity IDs.
        """
        loop = asyncio.get_running_loop()
        executor = self._get_executor()
        return await loop.run_in_executor(executor, self.get_neighbors, entity_id)

    def get_entity_contexts(self, entities: Sequence[str]) -> List[List[str]]:
        """
        @brief è·å–ä¸€ç»„å®ä½“å„è‡ªçš„ä¸€è·³é‚»å±…åˆ—è¡¨ï¼ˆå®ä½“ä¸Šä¸‹æ–‡ï¼‰ã€‚
               Get 1-hop neighbors (entity contexts) for a list of entities.
        @param entities Wikidata å®ä½“ ID åºåˆ—ã€‚Sequence of Wikidata entity IDs.
        @return æ¯ä¸ªå®ä½“å¯¹åº”çš„é‚»å±… ID åˆ—è¡¨ï¼Œé¡ºåºä¸è¾“å…¥å¯¹é½ã€‚
                List of neighbor ID lists, aligned with input order.
        """
        if not entities:
            return []

        # 1) å¯¹å®ä½“åˆ—è¡¨åšâ€œç¨³å®šå»é‡â€ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®ï¼Œæ„é€ æ˜ å°„ï¼š
        #    - unique_entities: å»é‡åçš„å®ä½“åˆ—è¡¨
        #    - index_map[i] = åŸå§‹ä½ç½® i å¯¹åº”åœ¨ unique_entities é‡Œçš„ç´¢å¼•
        unique_entities: List[str] = []
        index_map: List[int] = []
        seen: Dict[str, int] = {}

        for eid in entities:
            if eid in seen:
                idx = seen[eid]
            else:
                idx = len(unique_entities)
                unique_entities.append(eid)
                seen[eid] = idx
            index_map.append(idx)

        # 2) å¯¹å»é‡åçš„å®ä½“åˆ—è¡¨å¹¶å‘è°ƒç”¨ get_neighbors
        executor = self._get_executor()
        futures: List[Future[List[str]]] = []
        for eid in unique_entities:
            futures.append(executor.submit(self.get_neighbors, eid))

        unique_contexts: List[List[str]] = []
        for eid, fut in zip(unique_entities, futures):
            try:
                neighbors = fut.result()
            except Exception as e:  # noqa: BLE001
                logger.error("get_entity_contexts: failed for %r: %s", eid, e)
                neighbors = []
            unique_contexts.append(neighbors)

        # 3) æŒ‰åŸå§‹é¡ºåºè¿˜åŸï¼šå¯¹äºåŸæ¥çš„ç¬¬ i ä¸ªå®ä½“ï¼Œå– unique_contexts[index_map[i]]
        contexts: List[List[str]] = []
        for idx in index_map:
            contexts.append(unique_contexts[idx])

        return contexts

    async def aget_entity_contexts(self, entities: Sequence[str]) -> List[List[str]]:
        """
        @brief å¼‚æ­¥æ‰¹é‡è·å–å®ä½“ä¸Šä¸‹æ–‡ï¼ˆå¤šå®ä½“é‚»å±…åˆ—è¡¨ï¼‰ã€‚
               Async version of get_entity_contexts for a batch of entities.
        @param entities å®ä½“ ID åºåˆ—ã€‚Sequence of entity IDs.
        @return é‚»å±…å®ä½“ ID åˆ—è¡¨åºåˆ—ã€‚Sequence of neighbor ID lists.
        """
        if not entities:
            return []

        loop = asyncio.get_running_loop()
        executor = self._get_executor()

        tasks = [
            loop.run_in_executor(executor, self.get_neighbors, eid) for eid in entities
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        contexts: List[List[str]] = []
        for eid, res in zip(entities, results):
            if isinstance(res, Exception):
                logger.error("aget_entity_contexts: failed for %r: %s", eid, res)
                contexts.append([])
            else:
                contexts.append(res)
        return contexts
